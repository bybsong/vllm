version: '3.8'

# Network architecture following vLLM best practices
# - internal: NO internet access (production serving)
# - external: Has internet (for initial setup only)
# - default: Docker's default bridge (allows port exposure)
# - llamaindex_internal: Shared network with LlamaIndex (external reference)
networks:
  vllm-internal:
    driver: bridge
    internal: true  # NO internet access - secure production network
  vllm-external:
    driver: bridge  # Has internet for downloads
  llamaindex_internal:
    external: true
    name: llamaindex_internal
  default:
    # Uses docker-compose's default bridge network (allows port exposure)

services:
  # =========================================================================
  # MODEL DOWNLOADER (ONE-TIME USE)
  # Downloads models with internet access, then exits
  # Run once: docker-compose up model-downloader
  # =========================================================================
  model-downloader:
    image: alpine:latest
    container_name: vllm-model-downloader
    networks:
      - vllm-external  # Has internet for downloads
    volumes:
      - C:\Users\bybso\vllm\models:/models
    environment:
      - MODEL_NAME=nanonets/Nanonets-OCR2-3B
    command: |
      sh -c '
        echo "============================================"
        echo "vLLM Model Downloader"
        echo "============================================"
        echo ""
        echo "Installing git and git-lfs..."
        apk add --no-cache git git-lfs curl
        
        cd /models
        
        # Check if model already exists
        if [ -d "nanonets--Nanonets-OCR2-3B" ]; then
          echo "Model already downloaded!"
          echo "Location: /models/nanonets--Nanonets-OCR2-3B"
          exit 0
        fi
        
        echo ""
        echo "Downloading model: $$MODEL_NAME"
        echo "This may take 10-20 minutes (6-8GB)..."
        echo ""
        
        git lfs install
        GIT_LFS_SKIP_SMUDGE=0 git clone https://huggingface.co/$$MODEL_NAME nanonets--Nanonets-OCR2-3B
        
        echo ""
        echo "============================================"
        echo "✓ Model downloaded successfully!"
        echo "============================================"
        echo ""
        echo "Location: /models/nanonets--Nanonets-OCR2-3B"
        echo "Size: $$(du -sh nanonets--Nanonets-OCR2-3B | cut -f1)"
        echo ""
        echo "You can now start the serving containers:"
        echo "  docker-compose up -d vllm-nanonets-ocr nginx-gateway"
        echo ""
      '
    restart: "no"  # Run once and exit
    profiles:
      - setup  # Only runs when explicitly called

  # =========================================================================
  # OCR MODEL (Nanonets-OCR2-3B)
  # Vision model for document OCR
  # Access via: http://localhost:8001/ocr/v1/...
  # =========================================================================
  vllm-ocr:
    image: vllm/vllm-openai:latest
    container_name: vllm-ocr
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models:/root/.cache/huggingface
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model /root/.cache/huggingface/nanonets--Nanonets-OCR2-3B
      --served-model-name nanonets/Nanonets-OCR2-3B
      --host 0.0.0.0
      --port 8000
      --max-model-len 15000
      --gpu-memory-utilization 0.45
      --trust-remote-code
      --max-num-seqs 5
      --enforce-eager
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - ocr

  # =========================================================================
  # CHANDRA OCR MODEL (datalab-to/chandra)
  # Advanced vision OCR with superior table/handwriting recognition
  # Access via: http://localhost:8001/chandra/v1/...
  # =========================================================================
  vllm-chandra:
    image: vllm/vllm-openai:latest
    container_name: vllm-chandra
    networks:
      - default              # TEMP: Allow internet to download model
      # - vllm-internal      # NO internet access (internal=true) - ENABLE AFTER FIRST RUN
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models:/root/.cache/huggingface
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      # - HF_HUB_OFFLINE=1      # TEMP DISABLED - allow download
      # - TRANSFORMERS_OFFLINE=1  # TEMP DISABLED - allow download
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model datalab-to/chandra
      --served-model-name datalab-to/chandra
      --host 0.0.0.0
      --port 8000
      --max-model-len 32768
      --gpu-memory-utilization 0.45
      --trust-remote-code
      --max-num-seqs 8
      --enforce-eager
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - chandra

  # =========================================================================
  # TEXT MODEL (Qwen3-4B-Instruct)
  # Latest Qwen3 text-only model - fast and efficient
  # Access via: http://localhost:8003/v1/...
  # =========================================================================
  vllm-text:
    image: vllm/vllm-openai:latest
    container_name: vllm-text
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models\hub:/root/.cache/huggingface/hub
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model Qwen/Qwen3-4B-Instruct-2507
      --served-model-name Qwen/Qwen3-4B-Instruct-2507
      --host 0.0.0.0
      --port 8000
      --max-model-len 32768
      --gpu-memory-utilization 0.6
      --enforce-eager
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - text

  # =========================================================================
  # TEXT MODEL (Qwen3-30B-A3B-Instruct-FP8)
  # High-quality MoE text model with FP8 quantization
  # 30.5B total params, 3.3B active - faster than 14B dense!
  # FP8 optimized for RTX 5090 Blackwell architecture
  # Access via: vllm-text-30b:8000 (internal only)
  # =========================================================================
  vllm-text-30b:
    image: vllm/vllm-openai:latest
    container_name: vllm-text-30b
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models:/root/.cache/huggingface
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model /root/.cache/huggingface/Qwen--Qwen3-30B-A3B-Instruct-2507-FP8
      --served-model-name Qwen/Qwen3-30B-A3B-Instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 16384
      --gpu-memory-utilization 0.80
      --enforce-eager
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - text-30b

  # =========================================================================
  # TEXT MODEL (Qwen3-14B-Instruct) - NOT CONFIGURED
  # This model doesn't fit in 32GB VRAM at full precision
  # Use Qwen3-30B-A3B-FP8 (above) instead - it's faster and better!
  # =========================================================================
  vllm-text-14b:
    image: vllm/vllm-openai:latest
    container_name: vllm-text-14b
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models:/root/.cache/huggingface
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model /root/.cache/huggingface/Qwen--Qwen3-14B
      --served-model-name Qwen/Qwen3-14B
      --host 0.0.0.0
      --port 8000
      --max-model-len 16384
      --gpu-memory-utilization 0.75
      --enforce-eager
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    profiles:
      - text-14b

  # =========================================================================
  # QWEN3-VL MODEL (Qwen3-VL-30B-A3B-Instruct)
  # Latest-generation vision-language model with MoE architecture
  # 30.5B total params, only 3.3B active (~8-10GB VRAM)
  # Access via: http://localhost:8001/qwen3vl/v1/...
  # =========================================================================
  vllm-qwen3vl:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen3vl
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models\hub:/root/.cache/huggingface/hub
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ
      --served-model-name Qwen/Qwen3-VL-30B-A3B-Instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 8192
      --gpu-memory-utilization 0.92
      --trust-remote-code
      --max-num-seqs 5
      --quantization awq
      --enable-prefix-caching
      --max-num-batched-tokens 8192
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - qwen3vl

  # =========================================================================
  # HUNYUAN OCR MODEL (tencent/HunyuanOCR)
  # End-to-end OCR parsing (1B params, lightweight)
  # Access via: http://localhost:8006/v1/...
  # =========================================================================
  vllm-hunyuan-ocr:
    build:
      context: .
      dockerfile: docker/Dockerfile.nightly
    image: vllm-nightly:local
    container_name: vllm-hunyuan-ocr
    ports:
      - "8006:8000"  # Temporary direct access for testing
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models\hub:/root/.cache/huggingface/hub
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model tencent/HunyuanOCR
      --served-model-name tencent/HunyuanOCR
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --no-enable-prefix-caching
      --mm-processor-cache-gb 0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    profiles:
      - hunyuan-ocr

  # =========================================================================
  # QWEN3-VL-4B MODEL - STANDALONE MODE (High Performance)
  # Optimized for maximum performance when running alone
  # Uses 24GB VRAM (75% utilization) - NOT compatible with marker
  # Access via: http://localhost:8005/v1/...
  # Usage: docker-compose --profile qwen3vl-4b up -d vllm-qwen3vl-4b
  # =========================================================================
  vllm-qwen3vl-4b:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen3vl-4b
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models\hub:/root/.cache/huggingface/hub
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model Qwen/Qwen3-VL-4B-Instruct
      --served-model-name Qwen/Qwen3-VL-4B-Instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 8192
      --gpu-memory-utilization 0.75
      --trust-remote-code
      --max-num-seqs 3
      --enable-prefix-caching
      --max-num-batched-tokens 8192
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - qwen3vl-4b

  # =========================================================================
  # QWEN3-VL-4B MODEL - SHARED MODE (Memory Efficient)
  # Optimized for running alongside marker container
  # Uses 16GB VRAM (50% utilization) - Compatible with marker (5-6GB) + working memory (8GB)
  # Trade-offs: 4K context (vs 8K), 2 concurrent requests (vs 3)
  # Access via: http://localhost:8005/v1/...
  # Usage: docker-compose --profile qwen3vl-4b-shared up -d vllm-qwen3vl-4b-shared
  # =========================================================================
  vllm-qwen3vl-4b-shared:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen3vl-4b
    networks:
      - vllm-internal        # NO internet access (internal=true)
      - llamaindex_internal  # Accessible from LlamaIndex
    volumes:
      - C:\Users\bybso\vllm\models\hub:/root/.cache/huggingface/hub
    environment:
      - HF_HUB_DISABLE_TELEMETRY=1
      - DO_NOT_TRACK=1
      - VLLM_USAGE_SOURCE=production-docker-isolated
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: >
      --model Qwen/Qwen3-VL-4B-Instruct
      --served-model-name Qwen/Qwen3-VL-4B-Instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.50
      --trust-remote-code
      --max-num-seqs 2
      --enable-prefix-caching
      --max-num-batched-tokens 4096
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - qwen3vl-4b-shared

  # =========================================================================
  # NGINX GATEWAY (EXTERNAL ACCESS POINT)
  # Single gateway for all vLLM models
  # Routes traffic based on URL path:
  #   - /ocr/v1/*  → vllm-ocr
  #   - /text/v1/* → vllm-text
  # =========================================================================
  nginx-gateway:
    image: nginx:alpine
    container_name: vllm-gateway
    ports:
      - "8001:80"  # Single port for all models
    networks:
      - default        # Allows port exposure to host
      - vllm-internal  # Can talk to vllm containers
    volumes:
      - C:\Users\bybso\vllm\nginx-config\nginx-multi-model.conf:/etc/nginx/conf.d/default.conf:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3
