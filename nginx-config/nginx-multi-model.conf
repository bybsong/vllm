# Multi-Model Nginx Gateway for vLLM
# Routes different models based on URL path
# Architecture: Single gateway â†’ Multiple vLLM backends

# ============================================================================
# UPSTREAM BACKENDS
# ============================================================================

# OCR Model Backend (Nanonets-OCR2-3B)
upstream vllm_ocr {
    least_conn;  # Load balancing method
    server vllm-ocr:8000 max_fails=3 fail_timeout=30s;
    # Add more replicas if needed:
    # server vllm-ocr-2:8000 max_fails=3 fail_timeout=30s;
}

# Text Generation Model Backend (Qwen2.5-1.5B-Instruct)
upstream vllm_text {
    least_conn;
    server vllm-text:8000 max_fails=3 fail_timeout=30s;
    # Add more replicas if needed:
    # server vllm-text-2:8000 max_fails=3 fail_timeout=30s;
}

# ============================================================================
# MAIN SERVER
# ============================================================================

server {
    listen 80;
    server_name _;

    # Increase timeouts for long-running requests
    proxy_connect_timeout 300s;
    proxy_send_timeout 300s;
    proxy_read_timeout 300s;
    send_timeout 300s;

    # Increase body size for large images/documents
    client_max_body_size 50M;

    # ========================================================================
    # OCR MODEL ENDPOINTS
    # Path: /ocr/v1/*
    # ========================================================================
    location /ocr/ {
        # Remove /ocr prefix before forwarding
        rewrite ^/ocr/(.*) /$1 break;
        
        proxy_pass http://vllm_ocr;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Disable buffering for streaming responses
        proxy_buffering off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # Add custom header to identify backend
        add_header X-Model-Type ocr always;
        add_header X-Model-Name "nanonets/Nanonets-OCR2-3B" always;
    }

    # ========================================================================
    # TEXT MODEL ENDPOINTS
    # Path: /text/v1/*
    # ========================================================================
    location /text/ {
        # Remove /text prefix before forwarding
        rewrite ^/text/(.*) /$1 break;
        
        proxy_pass http://vllm_text;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Disable buffering for streaming responses
        proxy_buffering off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # Add custom header to identify backend
        add_header X-Model-Type text always;
        add_header X-Model-Name "Qwen/Qwen2.5-1.5B-Instruct" always;
    }

    # ========================================================================
    # HEALTH CHECK ENDPOINT
    # Returns status of all backends
    # ========================================================================
    location /health {
        access_log off;
        
        # Return simple health check
        # In production, you might want to check backend health
        return 200 "vLLM Multi-Model Gateway - Healthy\n";
        add_header Content-Type text/plain;
    }

    # ========================================================================
    # HEALTH CHECK FOR INDIVIDUAL BACKENDS
    # ========================================================================
    location /health/ocr {
        access_log off;
        rewrite ^/health/ocr$ /health break;
        proxy_pass http://vllm_ocr;
    }

    location /health/text {
        access_log off;
        rewrite ^/health/text$ /health break;
        proxy_pass http://vllm_text;
    }

    # ========================================================================
    # METRICS ENDPOINTS (Optional - for monitoring)
    # ========================================================================
    location /metrics/ocr {
        access_log off;
        rewrite ^/metrics/ocr$ /metrics break;
        proxy_pass http://vllm_ocr;
    }

    location /metrics/text {
        access_log off;
        rewrite ^/metrics/text$ /metrics break;
        proxy_pass http://vllm_text;
    }

    # ========================================================================
    # ROOT ENDPOINT - API Information
    # ========================================================================
    location = / {
        default_type application/json;
        return 200 '{
            "service": "vLLM Multi-Model Gateway",
            "version": "1.0",
            "models": {
                "ocr": {
                    "name": "nanonets/Nanonets-OCR2-3B",
                    "endpoint": "/ocr/v1/",
                    "type": "vision",
                    "health": "/health/ocr"
                },
                "text": {
                    "name": "Qwen/Qwen2.5-1.5B-Instruct",
                    "endpoint": "/text/v1/",
                    "type": "text",
                    "health": "/health/text"
                }
            },
            "endpoints": {
                "ocr_chat": "/ocr/v1/chat/completions",
                "ocr_completions": "/ocr/v1/completions",
                "text_chat": "/text/v1/chat/completions",
                "text_completions": "/text/v1/completions",
                "health": "/health",
                "metrics_ocr": "/metrics/ocr",
                "metrics_text": "/metrics/text"
            }
        }\n';
    }

    # ========================================================================
    # LOGGING
    # ========================================================================
    access_log /var/log/nginx/vllm_access.log;
    error_log /var/log/nginx/vllm_error.log;
}

